{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from utils import seed_everything, to_var, count_parameters, show_image_grid\n",
    "# from models import VAE\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "gc.collect()\n",
    "# CONST TABLE\n",
    "MPS_FLAG = torch.backends.mps.is_available()\n",
    "if MPS_FLAG:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Backend device: {}\".format(device))\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "ENC_HID_DIM = [4, 8, 16, 32]\n",
    "DEC_HID_DIM = [8, 32, 16, 8]\n",
    "LATENT_DIM = 16\n",
    "PRINT_FREQ = 100\n",
    "EPOCHES = 15\n",
    "KL_WEIGHT = 0.002\n",
    "LEARNING_RATE = 2e-4\n",
    "data_path = \"../assets/mnist/\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset1 = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST(data_path, train=False, transform=transform)\n",
    "train_loader = DataLoader(dataset1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset2, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        hidden_dims=[4, 8, 16, 32],\n",
    "        latent_dim=16,\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # input shape [b, 1, 28, 28]\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "        self.hidden_dims = hidden_dims\n",
    "        # for image processing we consider conv layers\n",
    "        modules = []\n",
    "        for h_dim in self.hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels=h_dim,\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(self.hidden_dims[-1] * 4, latent_dim)\n",
    "        self.fc_var = nn.Linear(self.hidden_dims[-1] * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
    "        o = self.encoder(x)\n",
    "        o = torch.flatten(o, start_dim=1)\n",
    "        mu = self.fc_mu(o)\n",
    "        log_var = self.fc_var(o)\n",
    "        return [mu, log_var]\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, hidden_dims: List[int] = [8, 32, 16, 8], latent_dim: int = 16\n",
    "    ) -> None:\n",
    "        super(ConvBlock, self).__init__()\n",
    "        conv_block = []\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            conv_block.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        hidden_dims[i],\n",
    "                        hidden_dims[i + 1],\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dims=[8, 32, 16, 8],\n",
    "        latent_dim=16,\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[0] * 49)\n",
    "        self.upsampling = nn.Upsample(scale_factor=4, mode=\"bilinear\")\n",
    "        self.conv_block1 = ConvBlock(hidden_dims, latent_dim)\n",
    "        self.conv_block2 = ConvBlock(hidden_dims, latent_dim)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dims[-1], out_channels=1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> List[torch.Tensor]:\n",
    "        o = self.decoder_input(z)\n",
    "        o = o.view(-1, self.hidden_dims[0], 7, 7)\n",
    "        o = self.upsampling(o)\n",
    "        o = self.conv_block1(o) + o\n",
    "        o = self.conv_block2(o) + o\n",
    "        o = self.output_layer(o)\n",
    "        return o\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_hidden_dim=[4, 8, 16, 32],\n",
    "        decoder_hidden_dim=[8, 32, 16, 8],\n",
    "        latent_dim=16,\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.enc = Encoder(\n",
    "            hidden_dims=encoder_hidden_dim, latent_dim=self.latent_dim\n",
    "        ).to(device)\n",
    "        self.dec = Decoder(\n",
    "            hidden_dims=decoder_hidden_dim, latent_dim=self.latent_dim\n",
    "        ).to(device)\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std, requires_grad=False)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def loss_function(self, xhat, x, mu, log_var, kl_weight=0.0025) -> dict:\n",
    "        recon_loss = F.mse_loss(xhat, x, reduction=\"mean\")\n",
    "        kld_loss = torch.mean(\n",
    "            -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0\n",
    "        )\n",
    "        loss = recon_loss + kl_weight * kld_loss\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"recon_loss\": recon_loss.detach(),\n",
    "            \"KLD\": kld_loss.detach(),\n",
    "        }\n",
    "\n",
    "    def forward(self, x) -> List[torch.Tensor]:\n",
    "        mu, log_var = self.enc(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        xhat = self.dec(z)\n",
    "        return [xhat, mu, log_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(\n",
    "    encoder_hidden_dim=ENC_HID_DIM,\n",
    "    decoder_hidden_dim=DEC_HID_DIM,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    device=device,\n",
    ").to(device)\n",
    "print(\"Encoder Parameters: {}\".format(count_parameters(model.enc)))\n",
    "print(\"Decoder Parameters: {}\".format(count_parameters(model.dec)))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "best_loss = 200\n",
    "for epoch in range(EPOCHES):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        xhat, mu, log_var = model(images)\n",
    "        # loss = F.mse_loss(images, xhat[0])\n",
    "        loss_dict = model.loss_function(\n",
    "            xhat=xhat, x=images, mu=mu, log_var=log_var, kl_weight=KL_WEIGHT\n",
    "        )\n",
    "        loss = loss_dict[\"loss\"]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            print(\n",
    "                \"[{:0>2d}][{}/{}]: loss: {:.4f}\\t rec_loss: {:.4f}\\t kld_loss: {:.4f}\\t Epoch time {:.3f}\".format(\n",
    "                    epoch,\n",
    "                    i,\n",
    "                    len(train_loader),\n",
    "                    loss.item(),\n",
    "                    loss_dict[\"recon_loss\"].item(),\n",
    "                    loss_dict[\"KLD\"].item(),\n",
    "                    time.time() - start_time,\n",
    "                )\n",
    "            )\n",
    "    model.eval()\n",
    "    loss_ = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            xhat, mu, log_var = model(images)\n",
    "            loss_ += model.loss_function(\n",
    "                xhat=xhat, x=images, mu=mu, log_var=log_var, kl_weight=KL_WEIGHT\n",
    "            )[\n",
    "                \"loss\"\n",
    "            ].item()  # F.mse_loss(images, xhat).item()\n",
    "        print(\n",
    "            \"[{:0>2d}]\\tValidation Loss: {:.4f}\".format(epoch, loss_ / len(test_loader))\n",
    "        )\n",
    "        if loss_ / len(test_loader) <= best_loss:\n",
    "            best_loss = loss_ / len(test_loader)\n",
    "            torch.save(model.state_dict(), \"./vae_nano.pt\")\n",
    "            print(\"Model Saved.\")\n",
    "# 0.0357, KL_WEIGHT = 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_restore = VAE(latent_dim=LATENT_DIM).to(device)\n",
    "model_restore.load_state_dict(torch.load(\"./vae_nano.pt\".format(LATENT_DIM)))\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(dataset1, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(dataset2, batch_size=BATCH_SIZE)\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    show_image_grid(images, BATCH_SIZE)\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        xhat = model_restore(images)\n",
    "        # print(xhat[1], xhat[2].exp())\n",
    "        show_image_grid(xhat[0].cpu(), BATCH_SIZE)\n",
    "    break\n",
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    show_image_grid(images, BATCH_SIZE)\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        xhat = model_restore(images)\n",
    "        # print(xhat[1], xhat[2].exp())\n",
    "        show_image_grid(xhat[0].cpu(), BATCH_SIZE)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xhat[1].shape,xhat[1].mean(),xhat[1].std())\n",
    "plt.imshow(xhat[1].cpu().detach().numpy(), cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xhat[2].shape,xhat[2].exp().mean(),xhat[2].exp().std())\n",
    "plt.imshow(xhat[2].exp().cpu().detach().numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
